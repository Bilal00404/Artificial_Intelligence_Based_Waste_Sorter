{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lfWq9ptdw9T"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khanhlvg/tflite_raspberry_pi/blob/main/object_detection/Train_custom_model_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpJEzDG6DK2Q"
      },
      "source": [
        "# Train a custom object detection model with TensorFlow Lite Model Maker\n",
        "\n",
        "The Model Maker library uses *transfer learning* to simplify the process of training a TensorFlow Lite model using a custom dataset. Retraining a TensorFlow Lite model with your own custom dataset reduces the amount of training data required and will shorten the training time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRYjtwRZGBOI"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "### Install the required packages\n",
        "Start by installing the required packages, including the Model Maker package from the [GitHub repo](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) and the pycocotools library you'll use for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35BJmtVpAP_n",
        "outputId": "cc173939-0cc6-4126-b43a-ce9bb6a13e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 577 kB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 840 kB 63.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 37.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 238 kB 62.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 60.9 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 2.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 38.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 32.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 7.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 55.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 128 kB 73.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 497.9 MB 4.2 kB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 71.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 64.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 58.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 52.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 216 kB 73.1 MB/s \n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tflite-model-maker\n",
        "!pip install -q tflite-support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prQ86DdtD317"
      },
      "source": [
        "Import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4QQTXHHATDS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat, QuantizationConfig\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "from tflite_support import metadata\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g6aQvXsD78P"
      },
      "source": [
        "### Prepare the dataset\n",
        "\n",
        "This dataset contains about 1000 images of 2 types of Waste: a recyclable waste and a Non-recyclable waste.\n",
        "\n",
        "We start with downloading the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1EiSutmffr_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc48fad4-275a-4a7d-9e61-77b67a12b641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting roboflow\n",
            "  Downloading roboflow-0.2.20-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 546 kB/s \n",
            "\u001b[?25hCollecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting python-dotenv\n",
            "  Downloading python_dotenv-0.21.0-py3-none-any.whl (18 kB)\n",
            "Collecting chardet==4.0.0\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 34.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (6.0)\n",
            "Collecting certifi==2021.5.30\n",
            "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 60.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (7.1.2)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.5.1.48 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.6.0.66)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.4.4)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.7)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.15.0)\n",
            "Collecting requests-toolbelt\n",
            "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting urllib3==1.26.6\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 57.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.23.0)\n",
            "Collecting cycler==0.10.0\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.3.1->roboflow) (4.1.1)\n",
            "Collecting requests\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->roboflow) (2.1.1)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=ab5add7baf4a6c88c052cc9c35fa6960586b516ed146c4d3f7897a8ad4494e87\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: urllib3, certifi, requests, pyparsing, cycler, wget, requests-toolbelt, python-dotenv, chardet, roboflow\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2022.9.24\n",
            "    Uninstalling certifi-2022.9.24:\n",
            "      Successfully uninstalled certifi-2022.9.24\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.11.0\n",
            "    Uninstalling cycler-0.11.0:\n",
            "      Successfully uninstalled cycler-0.11.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tflite-model-maker 0.4.2 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.6 which is incompatible.\u001b[0m\n",
            "Successfully installed certifi-2021.5.30 chardet-4.0.0 cycler-0.10.0 pyparsing-2.4.7 python-dotenv-0.21.0 requests-2.28.1 requests-toolbelt-0.10.1 roboflow-0.2.20 urllib3-1.26.6 wget-3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "chardet",
                  "cycler",
                  "pyparsing",
                  "requests",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in Waste_data_6-1 to voc: 100% [67459528 / 67459528] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to Waste_data_6-1 in voc:: 100%|██████████| 1877/1877 [00:01<00:00, 1534.01it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"HekdMCnHKWWTKkVpmYh2\")\n",
        "project = rf.workspace(\"sukkur-iba-university-dxckm\").project(\"waste_data_6\")\n",
        "dataset = project.version(1).download(\"voc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxh3KInCFeB-"
      },
      "source": [
        "## Train the object detection model\n",
        "\n",
        "### Step 1: Load the dataset\n",
        "\n",
        "* Images in `train_data` is used to train the custom object detection model.\n",
        "* Images in `val_data` is used to check if the model can generalize well to new images that it hasn't seen before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiAahdsQAdT7"
      },
      "outputs": [],
      "source": [
        "train_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    '/content/Waste_data_6-1/valid',\n",
        "    '/content/Waste_data_6-1/valid',\n",
        "    ['R', 'NR']\n",
        ")\n",
        "\n",
        "val_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    '/content/Waste_data_6-1/train',\n",
        "    '/content/Waste_data_6-1/train',\n",
        "    ['R', 'NR']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNRhB8N7GHXj"
      },
      "source": [
        "### Step 2: Select a model architecture\n",
        "\n",
        "EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture.\n",
        "\n",
        "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
        "\n",
        "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
        "|--------------------|-----------|---------------|----------------------|\n",
        "| EfficientDet-Lite0 | 4.4       | 146           | 25.69%               |\n",
        "| EfficientDet-Lite1 | 5.8       | 259           | 30.55%               |\n",
        "| EfficientDet-Lite2 | 7.2       | 396           | 33.97%               |\n",
        "| EfficientDet-Lite3 | 11.4      | 716           | 37.70%               |\n",
        "| EfficientDet-Lite4 | 19.9      | 1886          | 41.96%               |\n",
        "\n",
        "<i> * Size of the integer quantized models. <br/>\n",
        "** Latency measured on Raspberry Pi 4 using 4 threads on CPU. <br/>\n",
        "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
        "</i>\n",
        "\n",
        "In this notebook, we use EfficientDet-Lite0 to train our model. You can choose other model architectures depending on whether speed or accuracy is more important to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZOojrDHAY1J"
      },
      "outputs": [],
      "source": [
        "spec = model_spec.get('efficientdet_lite0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aeDU4mIM4ft"
      },
      "source": [
        "### Step 3: Train the TensorFlow model with the training data.\n",
        "\n",
        "* Set `epochs = 20`, which means it will go through the training dataset 20 times. You can look at the validation accuracy during training and stop when you see validation loss (`val_loss`) stop decreasing to avoid overfitting.\n",
        "* Set `batch_size = 4` here so you will see that it takes 15 steps to go through the 62 images in the training dataset.\n",
        "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MClfpsJAfda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "523d28a6-f00c-419e-ef5c-2b8bda65e9ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "186/186 [==============================] - 85s 248ms/step - det_loss: 1.0734 - cls_loss: 0.6388 - box_loss: 0.0087 - reg_l2_loss: 0.0631 - loss: 1.1366 - learning_rate: 0.0065 - gradient_norm: 5.0296 - val_det_loss: 1.0846 - val_cls_loss: 0.7647 - val_box_loss: 0.0064 - val_reg_l2_loss: 0.0634 - val_loss: 1.1480\n",
            "Epoch 2/20\n",
            "186/186 [==============================] - 44s 234ms/step - det_loss: 0.5979 - cls_loss: 0.3519 - box_loss: 0.0049 - reg_l2_loss: 0.0635 - loss: 0.6614 - learning_rate: 0.0049 - gradient_norm: 4.9610 - val_det_loss: 0.8072 - val_cls_loss: 0.4663 - val_box_loss: 0.0068 - val_reg_l2_loss: 0.0636 - val_loss: 0.8708\n",
            "Epoch 3/20\n",
            "186/186 [==============================] - 46s 249ms/step - det_loss: 0.5293 - cls_loss: 0.3198 - box_loss: 0.0042 - reg_l2_loss: 0.0637 - loss: 0.5930 - learning_rate: 0.0048 - gradient_norm: 4.8672 - val_det_loss: 0.7475 - val_cls_loss: 0.4313 - val_box_loss: 0.0063 - val_reg_l2_loss: 0.0638 - val_loss: 0.8113\n",
            "Epoch 4/20\n",
            "186/186 [==============================] - 44s 239ms/step - det_loss: 0.4822 - cls_loss: 0.2929 - box_loss: 0.0038 - reg_l2_loss: 0.0639 - loss: 0.5461 - learning_rate: 0.0046 - gradient_norm: 4.6015 - val_det_loss: 0.6236 - val_cls_loss: 0.3654 - val_box_loss: 0.0052 - val_reg_l2_loss: 0.0639 - val_loss: 0.6875\n",
            "Epoch 5/20\n",
            "186/186 [==============================] - 72s 386ms/step - det_loss: 0.4483 - cls_loss: 0.2787 - box_loss: 0.0034 - reg_l2_loss: 0.0640 - loss: 0.5123 - learning_rate: 0.0043 - gradient_norm: 4.4988 - val_det_loss: 0.4269 - val_cls_loss: 0.2631 - val_box_loss: 0.0033 - val_reg_l2_loss: 0.0640 - val_loss: 0.4909\n",
            "Epoch 6/20\n",
            "186/186 [==============================] - 46s 250ms/step - det_loss: 0.4151 - cls_loss: 0.2569 - box_loss: 0.0032 - reg_l2_loss: 0.0641 - loss: 0.4792 - learning_rate: 0.0040 - gradient_norm: 4.3487 - val_det_loss: 0.4166 - val_cls_loss: 0.2579 - val_box_loss: 0.0032 - val_reg_l2_loss: 0.0641 - val_loss: 0.4807\n",
            "Epoch 7/20\n",
            "186/186 [==============================] - 44s 237ms/step - det_loss: 0.4059 - cls_loss: 0.2529 - box_loss: 0.0031 - reg_l2_loss: 0.0642 - loss: 0.4701 - learning_rate: 0.0037 - gradient_norm: 4.1529 - val_det_loss: 0.2985 - val_cls_loss: 0.1892 - val_box_loss: 0.0022 - val_reg_l2_loss: 0.0642 - val_loss: 0.3627\n",
            "Epoch 8/20\n",
            "186/186 [==============================] - 45s 240ms/step - det_loss: 0.3785 - cls_loss: 0.2365 - box_loss: 0.0028 - reg_l2_loss: 0.0642 - loss: 0.4427 - learning_rate: 0.0033 - gradient_norm: 4.0358 - val_det_loss: 0.3679 - val_cls_loss: 0.2268 - val_box_loss: 0.0028 - val_reg_l2_loss: 0.0642 - val_loss: 0.4322\n",
            "Epoch 9/20\n",
            "186/186 [==============================] - 47s 251ms/step - det_loss: 0.3630 - cls_loss: 0.2236 - box_loss: 0.0028 - reg_l2_loss: 0.0643 - loss: 0.4272 - learning_rate: 0.0029 - gradient_norm: 3.7464 - val_det_loss: 0.2864 - val_cls_loss: 0.1807 - val_box_loss: 0.0021 - val_reg_l2_loss: 0.0643 - val_loss: 0.3507\n",
            "Epoch 10/20\n",
            "186/186 [==============================] - 67s 361ms/step - det_loss: 0.3543 - cls_loss: 0.2157 - box_loss: 0.0028 - reg_l2_loss: 0.0643 - loss: 0.4186 - learning_rate: 0.0025 - gradient_norm: 3.8483 - val_det_loss: 0.2753 - val_cls_loss: 0.1700 - val_box_loss: 0.0021 - val_reg_l2_loss: 0.0643 - val_loss: 0.3396\n",
            "Epoch 11/20\n",
            "186/186 [==============================] - 43s 232ms/step - det_loss: 0.3392 - cls_loss: 0.2112 - box_loss: 0.0026 - reg_l2_loss: 0.0643 - loss: 0.4035 - learning_rate: 0.0021 - gradient_norm: 3.6707 - val_det_loss: 0.2608 - val_cls_loss: 0.1635 - val_box_loss: 0.0019 - val_reg_l2_loss: 0.0643 - val_loss: 0.3251\n",
            "Epoch 12/20\n",
            "186/186 [==============================] - 46s 248ms/step - det_loss: 0.3316 - cls_loss: 0.2076 - box_loss: 0.0025 - reg_l2_loss: 0.0643 - loss: 0.3959 - learning_rate: 0.0017 - gradient_norm: 3.6104 - val_det_loss: 0.2392 - val_cls_loss: 0.1579 - val_box_loss: 0.0016 - val_reg_l2_loss: 0.0643 - val_loss: 0.3035\n",
            "Epoch 13/20\n",
            "186/186 [==============================] - 44s 236ms/step - det_loss: 0.3263 - cls_loss: 0.2021 - box_loss: 0.0025 - reg_l2_loss: 0.0643 - loss: 0.3906 - learning_rate: 0.0013 - gradient_norm: 3.7251 - val_det_loss: 0.2446 - val_cls_loss: 0.1586 - val_box_loss: 0.0017 - val_reg_l2_loss: 0.0643 - val_loss: 0.3089\n",
            "Epoch 14/20\n",
            "186/186 [==============================] - 43s 233ms/step - det_loss: 0.3121 - cls_loss: 0.1964 - box_loss: 0.0023 - reg_l2_loss: 0.0643 - loss: 0.3764 - learning_rate: 9.6624e-04 - gradient_norm: 3.6990 - val_det_loss: 0.2521 - val_cls_loss: 0.1606 - val_box_loss: 0.0018 - val_reg_l2_loss: 0.0643 - val_loss: 0.3164\n",
            "Epoch 15/20\n",
            "186/186 [==============================] - 69s 373ms/step - det_loss: 0.3177 - cls_loss: 0.2016 - box_loss: 0.0023 - reg_l2_loss: 0.0643 - loss: 0.3820 - learning_rate: 6.6281e-04 - gradient_norm: 3.7358 - val_det_loss: 0.2631 - val_cls_loss: 0.1711 - val_box_loss: 0.0018 - val_reg_l2_loss: 0.0643 - val_loss: 0.3274\n",
            "Epoch 16/20\n",
            "186/186 [==============================] - 44s 236ms/step - det_loss: 0.2922 - cls_loss: 0.1860 - box_loss: 0.0021 - reg_l2_loss: 0.0643 - loss: 0.3565 - learning_rate: 4.0950e-04 - gradient_norm: 3.4650 - val_det_loss: 0.2388 - val_cls_loss: 0.1531 - val_box_loss: 0.0017 - val_reg_l2_loss: 0.0643 - val_loss: 0.3031\n",
            "Epoch 17/20\n",
            "186/186 [==============================] - 44s 237ms/step - det_loss: 0.2954 - cls_loss: 0.1857 - box_loss: 0.0022 - reg_l2_loss: 0.0643 - loss: 0.3597 - learning_rate: 2.1321e-04 - gradient_norm: 3.5529 - val_det_loss: 0.2305 - val_cls_loss: 0.1520 - val_box_loss: 0.0016 - val_reg_l2_loss: 0.0643 - val_loss: 0.2948\n",
            "Epoch 18/20\n",
            "186/186 [==============================] - 44s 238ms/step - det_loss: 0.2934 - cls_loss: 0.1861 - box_loss: 0.0021 - reg_l2_loss: 0.0643 - loss: 0.3577 - learning_rate: 7.9291e-05 - gradient_norm: 3.4398 - val_det_loss: 0.2320 - val_cls_loss: 0.1520 - val_box_loss: 0.0016 - val_reg_l2_loss: 0.0643 - val_loss: 0.2963\n",
            "Epoch 19/20\n",
            "186/186 [==============================] - 46s 249ms/step - det_loss: 0.2938 - cls_loss: 0.1878 - box_loss: 0.0021 - reg_l2_loss: 0.0643 - loss: 0.3581 - learning_rate: 1.1406e-05 - gradient_norm: 3.4450 - val_det_loss: 0.2301 - val_cls_loss: 0.1517 - val_box_loss: 0.0016 - val_reg_l2_loss: 0.0643 - val_loss: 0.2944\n",
            "Epoch 20/20\n",
            "186/186 [==============================] - 66s 357ms/step - det_loss: 0.2892 - cls_loss: 0.1820 - box_loss: 0.0021 - reg_l2_loss: 0.0643 - loss: 0.3535 - learning_rate: 1.1405e-05 - gradient_norm: 3.3072 - val_det_loss: 0.2338 - val_cls_loss: 0.1524 - val_box_loss: 0.0016 - val_reg_l2_loss: 0.0643 - val_loss: 0.2981\n"
          ]
        }
      ],
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, batch_size=4, train_whole_model=True, epochs=20, validation_data=val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB4hKeerMmh4"
      },
      "source": [
        "### Step 4. Evaluate the model with the validation data.\n",
        "\n",
        "After training the object detection model using the images in the training dataset, use the 10 images in the validation dataset to evaluate how the model performs against new data it has never seen before.\n",
        "\n",
        "As the default batch size is 64, it will take 1 step to go through the 10 images in the validation dataset.\n",
        "\n",
        "The evaluation metrics are same as [COCO](https://cocodataset.org/#detection-eval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUqEpcYwAg8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b11d767-7f1a-4d7b-c0d8-347989115a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 29s 1s/step\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AP': 0.7263084,\n",
              " 'AP50': 0.9974865,\n",
              " 'AP75': 0.93036586,\n",
              " 'APs': -1.0,\n",
              " 'APm': -1.0,\n",
              " 'APl': 0.72634184,\n",
              " 'ARmax1': 0.7619148,\n",
              " 'ARmax10': 0.8024353,\n",
              " 'ARmax100': 0.80339754,\n",
              " 'ARs': -1.0,\n",
              " 'ARm': -1.0,\n",
              " 'ARl': 0.80339754,\n",
              " 'AP_/R': 0.701551,\n",
              " 'AP_/NR': 0.75106585}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model.evaluate(val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NARVYk9rGLIl"
      },
      "source": [
        "### Step 5: Export as a TensorFlow Lite model.\n",
        "\n",
        "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is [full integer quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant). This allows the TensorFlow Lite model to be smaller, run faster on Raspberry Pi CPU and also compatible with the Google Coral EdgeTPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u3eFxoBAiqE"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.', tflite_filename='Waste_Sorter_Final.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZcBmEigOCO3"
      },
      "source": [
        "### Step 6:  Evaluate the TensorFlow Lite model.\n",
        "\n",
        "Several factors can affect the model accuracy when exporting to TFLite:\n",
        "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop.\n",
        "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
        "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
        "\n",
        "Therefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbl8z9_wBPlr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "0db15e7a-eefe-47ce-905a-fd545c15f11a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  4/745 [..............................] - ETA: 30:08"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-303aff64b054>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_tflite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Waste_Sorter_Final.tflite'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py\u001b[0m in \u001b[0;36mevaluate_tflite\u001b[0;34m(self, tflite_filepath, data)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     return self.model_spec.evaluate_tflite(tflite_filepath, ds, len(data),\n\u001b[0;32m--> 156\u001b[0;31m                                            data.annotations_json_file)\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_export_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_model_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py\u001b[0m in \u001b[0;36mevaluate_tflite\u001b[0;34m(self, tflite_filepath, dataset, steps, json_file)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;31m# Get the output result after post-processing NMS op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlite_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;31m# CLASS_OFFSET is used since label_id for `background` is 0 in label_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/third_party/efficientdet/keras/eval_tflite.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_detail\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dtype'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m                                           self._subgraph_index)\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subgraph_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model.evaluate_tflite('Waste_Sorter_Final.tflite', val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7zgUkdOUUnD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "160778a0-8f9f-4aaa-a098-ad762737e743"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_53ce861d-5283-4879-a342-d34762804ebc\", \"Waste_Sorter_Final.tflite\", 4444711)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Download the TFLite model to your local computer.\n",
        "from google.colab import files\n",
        "files.download('Waste_Sorter_Final.tflite')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    '/content/Waste_data_6-1/test',\n",
        "    '/content/Waste_data_6-1/test',\n",
        "    ['R', 'NR']\n",
        ")"
      ],
      "metadata": {
        "id": "eW5H63wLx9yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate_tflite('Waste_Sorter_Final.tflite', test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGLXbcwwyA42",
        "outputId": "acf81644-f057-4786-96eb-2856076047a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51/51 [==============================] - 127s 2s/step\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AP': 0.58290285,\n",
              " 'AP50': 1.0,\n",
              " 'AP75': 0.4734074,\n",
              " 'APs': -1.0,\n",
              " 'APm': -1.0,\n",
              " 'APl': 0.58290285,\n",
              " 'ARmax1': 0.602,\n",
              " 'ARmax10': 0.609,\n",
              " 'ARmax100': 0.609,\n",
              " 'ARs': -1.0,\n",
              " 'ARm': -1.0,\n",
              " 'ARl': 0.609,\n",
              " 'AP_/R': 0.45049506,\n",
              " 'AP_/NR': 0.7153107}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}